{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from unity_environment_wrapper import unity_env, str_numbers\n",
    "from Multi_Agent_DDPG import Multi_MA_DDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_state6(self,state,prev_state=None):\n",
    "    state = np.reshape(state,(3,8))[:,0:6]\n",
    "    if prev_state is None:\n",
    "        state[0,:] = state[2,:]\n",
    "        state[1,:] = state[2,:]\n",
    "    else:\n",
    "        state[0,:] = prev_state[6:12]\n",
    "        state[1,:] = prev_state[12:]\n",
    "        #if abs(state[2,0]-state[1,0]) < 1e-6:\n",
    "        #    state[2,2] = 0.0\n",
    "        #if abs(state[2,1]-state[1,1]) < 1e-6:\n",
    "        #    state[2,3] = 0.0\n",
    "    return state.reshape(-1)\n",
    "\n",
    "def fix_state4(self,state,prev_state=None):\n",
    "    state = np.reshape(state,(3,8))[:,[0,1,4,5]]\n",
    "    if prev_state is None:\n",
    "        state[0,:] = state[2,:]\n",
    "        state[1,:] = state[2,:]\n",
    "    else:\n",
    "        state[0,:] = prev_state[4:8]\n",
    "        state[1,:] = prev_state[8:]\n",
    "    return state.reshape(-1)\n",
    "\n",
    "def modify_batch6(self):\n",
    "    if self.duplicate_batch > 1:\n",
    "        ind_start = self.batch_size\n",
    "        ind_end = self.batch_size*2\n",
    "        self.states[ind_start:ind_end,:,[4,10,16]] *= -1\n",
    "        self.next_states[ind_start:ind_end,:,[4,10,16]] *= -1\n",
    "        self.states[ind_start:ind_end,[0,1],:]      = self.states[ind_start:ind_end,[1,0],:]\n",
    "        self.actions[ind_start:ind_end,[0,1],:]     = self.actions[ind_start:ind_end,[1,0],:]\n",
    "        self.next_states[ind_start:ind_end,[0,1],:] = self.next_states[ind_start:ind_end,[1,0],:]\n",
    "        if self.rewards.shape[1] > 1:\n",
    "            self.rewards[ind_start:ind_end,[0,1]]   = self.rewards[ind_start:ind_end,[1,0]]\n",
    "        if self.dones.shape[1] > 1:\n",
    "            self.dones[ind_start:ind_end,[0,1]]     = self.dones[ind_start:ind_end,[1,0]]\n",
    "\n",
    "def modify_batch4(self):\n",
    "    if self.duplicate_batch > 1:\n",
    "        ind_start = self.batch_size\n",
    "        ind_end = self.batch_size*2\n",
    "        self.states[ind_start:ind_end,:,[2,6,10]] *= -1\n",
    "        self.next_states[ind_start:ind_end,:,[2,6,10]] *= -1\n",
    "        self.states[ind_start:ind_end,[0,1],:]      = self.states[ind_start:ind_end,[1,0],:]\n",
    "        self.actions[ind_start:ind_end,[0,1],:]     = self.actions[ind_start:ind_end,[1,0],:]\n",
    "        self.next_states[ind_start:ind_end,[0,1],:] = self.next_states[ind_start:ind_end,[1,0],:]\n",
    "        if self.rewards.shape[1] > 1:\n",
    "            self.rewards[ind_start:ind_end,[0,1]]   = self.rewards[ind_start:ind_end,[1,0]]\n",
    "        if self.dones.shape[1] > 1:\n",
    "            self.dones[ind_start:ind_end,[0,1]]     = self.dones[ind_start:ind_end,[1,0]]\n",
    "\n",
    "def modify_batch4_abs(self):\n",
    "    np.abs(self.states[:,:,[2,6,10]], out=self.states[:,:,[2,6,10]])\n",
    "    np.abs(self.next_states[:,:,[2,6,10]], out=self.next_states[:,:,[2,6,10]])\n",
    "    if self.duplicate_batch > 1:\n",
    "        ind_start = self.batch_size\n",
    "        ind_end = self.batch_size*2\n",
    "        self.states[ind_start:ind_end,[0,1],:]      = self.states[ind_start:ind_end,[1,0],:]\n",
    "        self.actions[ind_start:ind_end,[0,1],:]     = self.actions[ind_start:ind_end,[1,0],:]\n",
    "        self.next_states[ind_start:ind_end,[0,1],:] = self.next_states[ind_start:ind_end,[1,0],:]\n",
    "        if self.rewards.shape[1] > 1:\n",
    "            self.rewards[ind_start:ind_end,[0,1]]   = self.rewards[ind_start:ind_end,[1,0]]\n",
    "        if self.dones.shape[1] > 1:\n",
    "            self.dones[ind_start:ind_end,[0,1]]     = self.dones[ind_start:ind_end,[1,0]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring Unity environment...\n",
      "Selected brain name:  TennisBrain\n",
      "Selected brain:       Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n",
      "Number of actions:    2\n",
      "Number of agents:     2\n",
      "States have shape:    (12,)\n",
      "State of agent 0 look like:\n",
      " [ -6.638  -1.500   6.001   6.000  -6.638  -1.500   6.001   6.000  -6.638  -1.500   6.001   6.000]\n",
      "State of agent 1 look like:\n",
      " [ -7.079  -1.500  -6.001   6.000  -7.079  -1.500  -6.001   6.000  -7.079  -1.500  -6.001   6.000]\n",
      "Initializing DDPG_Agent with PyTorch device named: cuda:0\n",
      "replay_buffer_size = 1000000\n",
      "replay_batch_size  = 128\n",
      "seed               = 1\n",
      "gamma              = 0.95\n",
      "tau                = 0.001\n",
      "update_every       = 4\n",
      "update_times       = 1\n",
      "lr_actor           = 1e-05\n",
      "lr_critic          = 1e-06\n",
      "actor_clip_grad    = None\n",
      "critic_clip_grad   = None\n",
      "noise_sigma        = 0.2\n",
      "noise_theta        = 0.1\n",
      "no_reward_value    = -0.02\n",
      "actor_arch         = ['b', 256, 'b', 'r', 256, 'b', 'r', 128, 'b', 'r']\n",
      "critic_arch        = [['b', 256, 'b', 'r', 256, 'b', 'r', 128], ['b', 128, 'b', 'r', 64], ['b', 'r', 256, 'b', 'r', 128, 'b', 'r', 1]]\n",
      "Initializing Actor to:\n",
      " ModuleListUtil(\n",
      "  (0): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=12, out_features=256, bias=True)\n",
      "  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): LeakyReLU(negative_slope=0.01)\n",
      "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): LeakyReLU(negative_slope=0.01)\n",
      "  (7): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (8): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (9): LeakyReLU(negative_slope=0.01)\n",
      "  (10): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "Initializing Critic state layers to:\n",
      " ModuleListUtil(\n",
      "  (0): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=12, out_features=256, bias=True)\n",
      "  (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): LeakyReLU(negative_slope=0.01)\n",
      "  (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): LeakyReLU(negative_slope=0.01)\n",
      "  (7): Linear(in_features=256, out_features=128, bias=True)\n",
      ")\n",
      "Initializing Critic action layers to:\n",
      " ModuleListUtil(\n",
      "  (0): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (3): LeakyReLU(negative_slope=0.01)\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      ")\n",
      "Initializing Critic combine layers to:\n",
      " ModuleListUtil(\n",
      "  (0): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (1): LeakyReLU(negative_slope=0.01)\n",
      "  (2): Linear(in_features=384, out_features=256, bias=True)\n",
      "  (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): LeakyReLU(negative_slope=0.01)\n",
      "  (5): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (7): LeakyReLU(negative_slope=0.01)\n",
      "  (8): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = unity_env(file_name=\"Tennis_Windows_x86_64/Tennis.exe\", no_graphics=True, score_goal=0.5, verbose_level=1,\\\n",
    "                set_modify_state=fix_state4)\n",
    "\n",
    "    \n",
    "agent = Multi_MA_DDPG(state_shape = env.state_shape, action_size = env.action_size, num_agents=env.num_agents,\\\n",
    "                       seed=1,\\\n",
    "                       replay_buffer_size = int(1e6),\\\n",
    "                       replay_batch_size =128,\\\n",
    "                      duplicate_batch=1,\\\n",
    "                      set_modify_batch=modify_batch4_abs,\\\n",
    "                       update_every=4,\\\n",
    "                       update_times=1,\\\n",
    "                       lr_actor=1e-5,\\\n",
    "                       lr_critic=1e-6,\\\n",
    "                       noise_sigma=0.2,\\\n",
    "                       noise_theta=0.1,\\\n",
    "                       no_reward_value = -0.02,\\\n",
    "                       actor_arch = ['b',256,'b','r',256,'b','r',128,'b','r'],\\\n",
    "                       critic_arch = [['b',256,'b','r',256,'b','r',128],['b',128,'b','r',64],['b','r',256,'b','r',128,'b','r',1]],\\\n",
    "                       gamma = 0.95,\\\n",
    "                       use_cuda=True, verbose_level=2)\n",
    "\n",
    "output_name = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DDPG agent:\n",
      "output_filename    = results\n",
      "noise_minimal      = 0.01\n",
      "noise_decay        = 0.995\n",
      "num_episode_search = 2500\n",
      "max_num_episodes   = 10000\n",
      "score_window_size  = 100\n",
      "Episode 100 curr= 0.10 \tAverage=0.028 Stdev=0.047 Composite=-0.019 Max=0.200 \tTotal=2060 Positives=32 Negatives=100 Zeros=1928 \n",
      "Saving checkpoint...\n",
      "Episode 102 curr= 0.10 \tAverage=0.029 Stdev=0.048 Composite=-0.019 Max=0.200 \tTotal=2107 Positives=33 Negatives=102 Zeros=1972 \n",
      "Saving checkpoint...\n",
      "Episode 111 curr= 0.10 \tAverage=0.028 Stdev=0.045 Composite=-0.017 Max=0.100 \tTotal=2254 Positives=34 Negatives=111 Zeros=2109 \n",
      "Saving checkpoint...\n",
      "Episode 115 curr= 0.09 \tAverage=0.029 Stdev=0.045 Composite=-0.017 Max=0.100 \tTotal=2347 Positives=36 Negatives=115 Zeros=2196 \n",
      "Saving checkpoint...\n",
      "Episode 116 curr= 0.10 \tAverage=0.030 Stdev=0.046 Composite=-0.016 Max=0.100 \tTotal=2375 Positives=37 Negatives=116 Zeros=2222 \n",
      "Saving checkpoint...\n",
      "Episode 333 curr= 0.10 \tAverage=0.033 Stdev=0.049 Composite=-0.016 Max=0.200 \tTotal=6974 Positives=105 Negatives=331 Zeros=6538 \n",
      "Saving checkpoint...\n",
      "Episode 337 curr= 0.10 \tAverage=0.034 Stdev=0.049 Composite=-0.015 Max=0.200 \tTotal=7067 Positives=107 Negatives=335 Zeros=6625 \n",
      "Saving checkpoint...\n",
      "Episode 354 curr= 0.09 \tAverage=0.034 Stdev=0.049 Composite=-0.015 Max=0.200 \tTotal=7406 Positives=111 Negatives=351 Zeros=6944 \n",
      "Saving checkpoint...\n",
      "Episode 366 curr= 0.09 \tAverage=0.035 Stdev=0.049 Composite=-0.014 Max=0.200 \tTotal=7637 Positives=114 Negatives=363 Zeros=7160 \n",
      "Saving checkpoint...\n",
      "Episode 373 curr= 0.10 \tAverage=0.035 Stdev=0.049 Composite=-0.014 Max=0.200 \tTotal=7774 Positives=116 Negatives=370 Zeros=7288 \n",
      "Saving checkpoint...\n",
      "Episode 374 curr= 0.10 \tAverage=0.036 Stdev=0.050 Composite=-0.013 Max=0.200 \tTotal=7807 Positives=117 Negatives=371 Zeros=7319 \n",
      "Saving checkpoint...\n",
      "Episode 375 curr= 0.10 \tAverage=0.037 Stdev=0.050 Composite=-0.013 Max=0.200 \tTotal=7840 Positives=118 Negatives=372 Zeros=7350 \n",
      "Saving checkpoint...\n",
      "Episode 487 curr= 0.10 \tAverage=0.037 Stdev=0.050 Composite=-0.013 Max=0.190 \tTotal=10335 Positives=160 Negatives=483 Zeros=9692 \n",
      "Saving checkpoint...\n",
      "Episode 492 curr= 0.10 \tAverage=0.038 Stdev=0.050 Composite=-0.012 Max=0.190 \tTotal=10425 Positives=161 Negatives=488 Zeros=9776 \n",
      "Saving checkpoint...\n",
      "Episode 495 curr= 0.10 \tAverage=0.039 Stdev=0.050 Composite=-0.011 Max=0.190 \tTotal=10492 Positives=162 Negatives=491 Zeros=9839 \n",
      "Saving checkpoint...\n",
      "Episode 497 curr= 0.10 \tAverage=0.040 Stdev=0.051 Composite=-0.010 Max=0.190 \tTotal=10539 Positives=163 Negatives=493 Zeros=9883 \n",
      "Saving checkpoint...\n",
      "Episode 498 curr= 0.09 \tAverage=0.041 Stdev=0.051 Composite=-0.009 Max=0.190 \tTotal=10572 Positives=164 Negatives=494 Zeros=9914 \n",
      "Saving checkpoint...\n",
      "Episode 499 curr= 0.10 \tAverage=0.042 Stdev=0.051 Composite=-0.009 Max=0.190 \tTotal=10623 Positives=166 Negatives=495 Zeros=9962 \n",
      "Saving checkpoint...\n",
      "Episode 501 curr= 0.09 \tAverage=0.042 Stdev=0.051 Composite=-0.009 Max=0.190 \tTotal=10670 Positives=167 Negatives=497 Zeros=10006 \n",
      "Saving checkpoint...\n",
      "Episode 505 curr= 0.10 \tAverage=0.042 Stdev=0.051 Composite=-0.009 Max=0.190 \tTotal=10764 Positives=169 Negatives=501 Zeros=10094 \n",
      "Saving checkpoint...\n",
      "Episode 510 curr= 0.09 \tAverage=0.042 Stdev=0.051 Composite=-0.009 Max=0.190 \tTotal=10890 Positives=171 Negatives=506 Zeros=10213 \n",
      "Saving checkpoint...\n",
      "Episode 614 curr= 0.09 \tAverage=0.041 Stdev=0.049 Composite=-0.008 Max=0.100 \tTotal=13669 Positives=234 Negatives=610 Zeros=12825 \n",
      "Saving checkpoint...\n",
      "Episode 630 curr= 0.10 \tAverage=0.042 Stdev=0.049 Composite=-0.007 Max=0.100 \tTotal=14026 Positives=241 Negatives=626 Zeros=13159 \n",
      "Saving checkpoint...\n",
      "Episode 631 curr= 0.10 \tAverage=0.043 Stdev=0.049 Composite=-0.006 Max=0.100 \tTotal=14059 Positives=242 Negatives=627 Zeros=13190 \n",
      "Saving checkpoint...\n",
      "Episode 646 curr= 0.10 \tAverage=0.043 Stdev=0.049 Composite=-0.006 Max=0.100 \tTotal=14382 Positives=248 Negatives=642 Zeros=13492 \n",
      "Saving checkpoint...\n",
      "Episode 819 curr= 0.09 \tAverage=0.043 Stdev=0.049 Composite=-0.006 Max=0.100 \tTotal=18440 Positives=327 Negatives=811 Zeros=17302 \n",
      "Saving checkpoint...\n",
      "Episode 821 curr= 0.10 \tAverage=0.044 Stdev=0.049 Composite=-0.005 Max=0.100 \tTotal=18486 Positives=328 Negatives=813 Zeros=17345 \n",
      "Saving checkpoint...\n",
      "Episode 1196 curr= 0.09 \tAverage=0.044 Stdev=0.049 Composite=-0.005 Max=0.100 \tTotal=27240 Positives=494 Negatives=1187 Zeros=25559 \n",
      "Saving checkpoint...\n",
      "Episode 1221 curr= 0.10 \tAverage=0.045 Stdev=0.049 Composite=-0.004 Max=0.100 \tTotal=27826 Positives=505 Negatives=1212 Zeros=26109 \n",
      "Saving checkpoint...\n",
      "Episode 1385 curr= 0.10 \tAverage=0.052 Stdev=0.055 Composite=-0.003 Max=0.300 \tTotal=31800 Positives=585 Negatives=1375 Zeros=29840 \n",
      "Saving checkpoint...\n",
      "Episode 1387 curr= 0.10 \tAverage=0.053 Stdev=0.055 Composite=-0.002 Max=0.300 \tTotal=31866 Positives=587 Negatives=1377 Zeros=29902 \n",
      "Saving checkpoint...\n",
      "Episode 1389 curr= 0.10 \tAverage=0.053 Stdev=0.055 Composite=-0.002 Max=0.300 \tTotal=31913 Positives=588 Negatives=1379 Zeros=29946 \n",
      "Saving checkpoint...\n",
      "Episode 1390 curr= 0.10 \tAverage=0.051 Stdev=0.049 Composite=0.002 Max=0.100 \tTotal=31945 Positives=589 Negatives=1380 Zeros=29976 \n",
      "Saving checkpoint...\n",
      "Episode 1391 curr= 0.10 \tAverage=0.052 Stdev=0.049 Composite=0.003 Max=0.100 \tTotal=31978 Positives=590 Negatives=1381 Zeros=30007 \n",
      "Saving checkpoint...\n",
      "Episode 1392 curr= 0.10 \tAverage=0.053 Stdev=0.049 Composite=0.004 Max=0.100 \tTotal=32010 Positives=591 Negatives=1382 Zeros=30037 \n",
      "Saving checkpoint...\n",
      "Episode 1393 curr= 0.10 \tAverage=0.053 Stdev=0.049 Composite=0.004 Max=0.100 \tTotal=32062 Positives=593 Negatives=1383 Zeros=30086 \n",
      "Saving checkpoint...\n",
      "Episode 1398 curr= 0.10 \tAverage=0.053 Stdev=0.049 Composite=0.004 Max=0.100 \tTotal=32187 Positives=595 Negatives=1388 Zeros=30204 \n",
      "Saving checkpoint...\n",
      "Episode 1399 curr= 0.10 \tAverage=0.054 Stdev=0.049 Composite=0.005 Max=0.100 \tTotal=32219 Positives=596 Negatives=1389 Zeros=30234 \n",
      "Saving checkpoint...\n",
      "Episode 1400 curr= 0.10 \tAverage=0.055 Stdev=0.049 Composite=0.006 Max=0.100 \tTotal=32252 Positives=597 Negatives=1390 Zeros=30265 \n",
      "Saving checkpoint...\n",
      "Episode 1401 curr= 0.10 \tAverage=0.055 Stdev=0.049 Composite=0.006 Max=0.100 \tTotal=32285 Positives=598 Negatives=1391 Zeros=30296 \n",
      "Saving checkpoint...\n",
      "Episode 1404 curr= 0.10 \tAverage=0.056 Stdev=0.049 Composite=0.007 Max=0.100 \tTotal=32365 Positives=600 Negatives=1394 Zeros=30371 \n",
      "Saving checkpoint...\n",
      "Episode 1406 curr= 0.10 \tAverage=0.057 Stdev=0.049 Composite=0.008 Max=0.100 \tTotal=32431 Positives=602 Negatives=1396 Zeros=30433 \n",
      "Saving checkpoint...\n",
      "Episode 1408 curr= 0.10 \tAverage=0.058 Stdev=0.049 Composite=0.009 Max=0.100 \tTotal=32478 Positives=603 Negatives=1398 Zeros=30477 \n",
      "Saving checkpoint...\n",
      "Episode 1409 curr= 0.10 \tAverage=0.059 Stdev=0.049 Composite=0.010 Max=0.100 \tTotal=32511 Positives=604 Negatives=1399 Zeros=30508 \n",
      "Saving checkpoint...\n",
      "Episode 1412 curr= 0.09 \tAverage=0.060 Stdev=0.048 Composite=0.012 Max=0.100 \tTotal=32572 Positives=605 Negatives=1402 Zeros=30565 \n",
      "Saving checkpoint...\n",
      "Episode 1417 curr= 0.10 \tAverage=0.061 Stdev=0.048 Composite=0.013 Max=0.100 \tTotal=32702 Positives=608 Negatives=1407 Zeros=30687 \n",
      "Saving checkpoint...\n",
      "Episode 1418 curr= 0.10 \tAverage=0.062 Stdev=0.048 Composite=0.014 Max=0.100 \tTotal=32735 Positives=609 Negatives=1408 Zeros=30718 \n",
      "Saving checkpoint...\n",
      "Episode 1422 curr= 0.10 \tAverage=0.062 Stdev=0.048 Composite=0.014 Max=0.100 \tTotal=32827 Positives=611 Negatives=1412 Zeros=30804 \n",
      "Saving checkpoint...\n",
      "Episode 1436 curr= 0.09 \tAverage=0.063 Stdev=0.048 Composite=0.015 Max=0.100 \tTotal=33177 Positives=619 Negatives=1426 Zeros=31132 \n",
      "Saving checkpoint...\n",
      "Episode 1437 curr= 0.09 \tAverage=0.064 Stdev=0.047 Composite=0.017 Max=0.100 \tTotal=33209 Positives=620 Negatives=1427 Zeros=31162 \n",
      "Saving checkpoint...\n",
      "Episode 1474 curr= 0.10 \tAverage=0.064 Stdev=0.047 Composite=0.017 Max=0.100 \tTotal=34249 Positives=645 Negatives=1464 Zeros=32140 \n",
      "Saving checkpoint...\n",
      "Episode 1666 curr= 0.10 \tAverage=0.065 Stdev=0.047 Composite=0.018 Max=0.100 \tTotal=39468 Positives=765 Negatives=1655 Zeros=37048 \n",
      "Saving checkpoint...\n",
      "Episode 1667 curr= 0.10 \tAverage=0.066 Stdev=0.046 Composite=0.019 Max=0.100 \tTotal=39501 Positives=766 Negatives=1656 Zeros=37079 \n",
      "Saving checkpoint...\n",
      "Episode 1668 curr= 0.10 \tAverage=0.066 Stdev=0.046 Composite=0.019 Max=0.100 \tTotal=39534 Positives=767 Negatives=1657 Zeros=37110 \n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1785 curr= 0.10 \tAverage=0.069 Stdev=0.049 Composite=0.020 Max=0.200 \tTotal=42912 Positives=852 Negatives=1772 Zeros=40288 \n",
      "Saving checkpoint...\n",
      "Episode 1787 curr= 0.09 \tAverage=0.070 Stdev=0.048 Composite=0.022 Max=0.200 \tTotal=42978 Positives=853 Negatives=1774 Zeros=40351 \n",
      "Saving checkpoint...\n",
      "Episode 1789 curr= 0.10 \tAverage=0.071 Stdev=0.048 Composite=0.023 Max=0.200 \tTotal=43042 Positives=854 Negatives=1776 Zeros=40412 \n",
      "Saving checkpoint...\n",
      "Episode 1790 curr= 0.10 \tAverage=0.071 Stdev=0.048 Composite=0.023 Max=0.200 \tTotal=43074 Positives=855 Negatives=1777 Zeros=40442 \n",
      "Saving checkpoint...\n",
      "Episode 1791 curr= 0.10 \tAverage=0.071 Stdev=0.048 Composite=0.023 Max=0.200 \tTotal=43107 Positives=856 Negatives=1778 Zeros=40473 \n",
      "Saving checkpoint...\n",
      "Episode 1792 curr= 0.09 \tAverage=0.072 Stdev=0.048 Composite=0.025 Max=0.200 \tTotal=43140 Positives=857 Negatives=1779 Zeros=40504 \n",
      "Saving checkpoint...\n",
      "Episode 1793 curr= 0.10 \tAverage=0.072 Stdev=0.048 Composite=0.025 Max=0.200 \tTotal=43173 Positives=858 Negatives=1780 Zeros=40535 \n",
      "Saving checkpoint...\n",
      "Episode 1794 curr= 0.10 \tAverage=0.073 Stdev=0.047 Composite=0.026 Max=0.200 \tTotal=43206 Positives=859 Negatives=1781 Zeros=40566 \n",
      "Saving checkpoint...\n",
      "Episode 1795 curr= 0.09 \tAverage=0.074 Stdev=0.047 Composite=0.028 Max=0.200 \tTotal=43238 Positives=860 Negatives=1782 Zeros=40596 \n",
      "Saving checkpoint...\n",
      "Episode 1802 curr= 0.10 \tAverage=0.075 Stdev=0.046 Composite=0.029 Max=0.200 \tTotal=43486 Positives=868 Negatives=1789 Zeros=40829 \n",
      "Saving checkpoint...\n",
      "Episode 1803 curr= 0.09 \tAverage=0.076 Stdev=0.045 Composite=0.030 Max=0.200 \tTotal=43519 Positives=869 Negatives=1790 Zeros=40860 \n",
      "Saving checkpoint...\n",
      "Episode 1804 curr= 0.09 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43551 Positives=870 Negatives=1791 Zeros=40890 \n",
      "Saving checkpoint...\n",
      "Episode 1808 curr= 0.10 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43664 Positives=873 Negatives=1795 Zeros=40996 \n",
      "Saving checkpoint...\n",
      "Episode 1811 curr= 0.10 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43743 Positives=875 Negatives=1798 Zeros=41070 \n",
      "Saving checkpoint...\n",
      "Episode 1812 curr= 0.09 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43775 Positives=876 Negatives=1799 Zeros=41100 \n",
      "Saving checkpoint...\n",
      "Episode 1813 curr= 0.10 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43808 Positives=877 Negatives=1800 Zeros=41131 \n",
      "Saving checkpoint...\n",
      "Episode 1817 curr= 0.10 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43939 Positives=880 Negatives=1804 Zeros=41255 \n",
      "Saving checkpoint...\n",
      "Episode 1818 curr= 0.10 \tAverage=0.077 Stdev=0.045 Composite=0.032 Max=0.200 \tTotal=43972 Positives=881 Negatives=1805 Zeros=41286 \n",
      "Saving checkpoint...\n",
      "Episode 1820 curr= 0.10 \tAverage=0.079 Stdev=0.046 Composite=0.033 Max=0.200 \tTotal=44056 Positives=885 Negatives=1806 Zeros=41365 \n",
      "Saving checkpoint...\n",
      "Episode 1825 curr= 0.10 \tAverage=0.079 Stdev=0.046 Composite=0.033 Max=0.200 \tTotal=44202 Positives=889 Negatives=1811 Zeros=41502 \n",
      "Saving checkpoint...\n",
      "Episode 1826 curr= 0.10 \tAverage=0.080 Stdev=0.045 Composite=0.035 Max=0.200 \tTotal=44234 Positives=890 Negatives=1812 Zeros=41532 \n",
      "Saving checkpoint...\n",
      "Episode 1827 curr= 0.10 \tAverage=0.080 Stdev=0.045 Composite=0.035 Max=0.200 \tTotal=44267 Positives=891 Negatives=1813 Zeros=41563 \n",
      "Saving checkpoint...\n",
      "Episode 1835 curr= 0.10 \tAverage=0.080 Stdev=0.045 Composite=0.035 Max=0.200 \tTotal=44506 Positives=897 Negatives=1821 Zeros=41788 \n",
      "Saving checkpoint...\n",
      "Episode 1837 curr= 0.10 \tAverage=0.081 Stdev=0.045 Composite=0.037 Max=0.200 \tTotal=44591 Positives=900 Negatives=1823 Zeros=41868 \n",
      "Saving checkpoint...\n",
      "Episode 1838 curr= 0.10 \tAverage=0.081 Stdev=0.045 Composite=0.037 Max=0.200 \tTotal=44624 Positives=901 Negatives=1824 Zeros=41899 \n",
      "Saving checkpoint...\n",
      "Episode 1855 curr= 0.10 \tAverage=0.082 Stdev=0.044 Composite=0.038 Max=0.200 \tTotal=45128 Positives=914 Negatives=1841 Zeros=42373 \n",
      "Saving checkpoint...\n",
      "Episode 1858 curr= 0.10 \tAverage=0.083 Stdev=0.043 Composite=0.040 Max=0.200 \tTotal=45225 Positives=917 Negatives=1844 Zeros=42464 \n",
      "Saving checkpoint...\n",
      "Episode 1971 curr= 0.10 \tAverage=0.079 Stdev=0.039 Composite=0.040 Max=0.100 \tTotal=48611 Positives=1009 Negatives=1957 Zeros=45645 \n",
      "Saving checkpoint...\n",
      "Episode 1976 curr= 0.10 \tAverage=0.080 Stdev=0.038 Composite=0.042 Max=0.100 \tTotal=48775 Positives=1014 Negatives=1962 Zeros=45799 \n",
      "Saving checkpoint...\n",
      "Episode 1980 curr= 0.10 \tAverage=0.080 Stdev=0.038 Composite=0.042 Max=0.100 \tTotal=48906 Positives=1018 Negatives=1966 Zeros=45922 \n",
      "Saving checkpoint...\n",
      "Episode 1981 curr= 0.10 \tAverage=0.081 Stdev=0.037 Composite=0.044 Max=0.100 \tTotal=48939 Positives=1019 Negatives=1967 Zeros=45953 \n",
      "Saving checkpoint...\n",
      "Episode 1982 curr= 0.09 \tAverage=0.082 Stdev=0.036 Composite=0.046 Max=0.100 \tTotal=48972 Positives=1020 Negatives=1968 Zeros=45984 \n",
      "Saving checkpoint...\n",
      "Episode 1984 curr= 0.10 \tAverage=0.083 Stdev=0.035 Composite=0.048 Max=0.100 \tTotal=49038 Positives=1022 Negatives=1970 Zeros=46046 \n",
      "Saving checkpoint...\n",
      "Episode 1985 curr= 0.10 \tAverage=0.084 Stdev=0.034 Composite=0.050 Max=0.100 \tTotal=49071 Positives=1023 Negatives=1971 Zeros=46077 \n",
      "Saving checkpoint...\n",
      "Episode 1986 curr= 0.10 \tAverage=0.084 Stdev=0.034 Composite=0.050 Max=0.100 \tTotal=49104 Positives=1024 Negatives=1972 Zeros=46108 \n",
      "Saving checkpoint...\n",
      "Episode 1999 curr= 0.10 \tAverage=0.085 Stdev=0.033 Composite=0.051 Max=0.100 \tTotal=49493 Positives=1035 Negatives=1985 Zeros=46473 \n",
      "Saving checkpoint...\n",
      "Episode 2004 curr= 0.10 \tAverage=0.086 Stdev=0.032 Composite=0.054 Max=0.100 \tTotal=49658 Positives=1040 Negatives=1990 Zeros=46628 \n",
      "Saving checkpoint...\n",
      "Episode 2005 curr= 0.10 \tAverage=0.086 Stdev=0.032 Composite=0.054 Max=0.100 \tTotal=49691 Positives=1041 Negatives=1991 Zeros=46659 \n",
      "Saving checkpoint...\n",
      "Episode 2006 curr= 0.10 \tAverage=0.086 Stdev=0.032 Composite=0.054 Max=0.100 \tTotal=49723 Positives=1042 Negatives=1992 Zeros=46689 \n",
      "Saving checkpoint...\n",
      "Episode 2007 curr= 0.10 \tAverage=0.087 Stdev=0.031 Composite=0.056 Max=0.100 \tTotal=49756 Positives=1043 Negatives=1993 Zeros=46720 \n",
      "Saving checkpoint...\n",
      "Episode 2009 curr= 0.10 \tAverage=0.087 Stdev=0.031 Composite=0.056 Max=0.100 \tTotal=49841 Positives=1046 Negatives=1995 Zeros=46800 \n",
      "Saving checkpoint...\n",
      "Episode 2016 curr= 0.10 \tAverage=0.087 Stdev=0.031 Composite=0.056 Max=0.100 \tTotal=50052 Positives=1052 Negatives=2002 Zeros=46998 \n",
      "Saving checkpoint...\n",
      "Episode 2017 curr= 0.10 \tAverage=0.087 Stdev=0.031 Composite=0.056 Max=0.100 \tTotal=50085 Positives=1053 Negatives=2003 Zeros=47029 \n",
      "Saving checkpoint...\n",
      "Episode 2026 curr= 0.10 \tAverage=0.088 Stdev=0.030 Composite=0.058 Max=0.100 \tTotal=50419 Positives=1062 Negatives=2010 Zeros=47347 \n",
      "Saving checkpoint...\n",
      "Episode 2027 curr= 0.10 \tAverage=0.089 Stdev=0.028 Composite=0.060 Max=0.100 \tTotal=50452 Positives=1063 Negatives=2011 Zeros=47378 \n",
      "Saving checkpoint...\n",
      "Episode 2038 curr= 0.10 \tAverage=0.090 Stdev=0.027 Composite=0.063 Max=0.100 \tTotal=50814 Positives=1074 Negatives=2022 Zeros=47718 \n",
      "Saving checkpoint...\n",
      "Episode 2040 curr= 0.10 \tAverage=0.090 Stdev=0.027 Composite=0.063 Max=0.100 \tTotal=50880 Positives=1076 Negatives=2024 Zeros=47780 \n",
      "Saving checkpoint...\n",
      "Episode 2064 curr= 0.10 \tAverage=0.088 Stdev=0.030 Composite=0.058 Max=0.100 \tTotal=51647 Positives=1098 Negatives=2048 Zeros=48501 "
     ]
    }
   ],
   "source": [
    "agent.reset()\n",
    "all_scores, best_test_score, best_test_average, best_test_stdev = env.train(agent,output_name,num_episode_search=2500,noise_decay=0.995)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax  = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(all_scores)), all_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Train Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load(output_name)\n",
    "test_composite, test_mean, test_stdev, test_scores = env.test_agent_on_many_episodes(agent)\n",
    "print(\"Final Test scores on {} episodes:\".format(len(test_scores)))\n",
    "print(\"Mean score:    \",test_mean)\n",
    "print(\"Std.Dev. score:\",test_stdev)\n",
    "\n",
    "env.test_agent_on_single_episode(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
